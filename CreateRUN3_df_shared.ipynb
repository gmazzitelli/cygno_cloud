{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5235fc33-3d8e-4166-ad52-ff0e872e3290",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<a id='TOC'></a>\n",
    "\n",
    "0. [Imports](#imports)\n",
    "1. [Run range and interesting keys](#runs)\n",
    "2. [Create Dataframe](#df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b63dda6-512c-494f-a598-92d178785c88",
   "metadata": {},
   "source": [
    "## Imports\n",
    "<a id = 'imports'></a>\n",
    "\n",
    "Go to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cfafdca-e274-405e-a788-8204c5c204a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cygno as cy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import uproot\n",
    "from tqdm.notebook import tqdm\n",
    "from time import process_time \n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "# custom params for plots\n",
    "plt.rcParams['figure.titlesize'] = 18\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = .5\n",
    "plt.rcParams['grid.linestyle'] = '--'\n",
    "plt.rcParams['figure.figsize'] = 12, 7\n",
    "plt.rcParams['figure.subplot.wspace'] = 0.2\n",
    "plt.rcParams['figure.subplot.hspace'] = 0.4\n",
    "\n",
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af1c354-a763-4865-8da3-fed775bc0fc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run range and interesting keys\n",
    "<a id = 'runs'></a>\n",
    "\n",
    "Go to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94064661-0b03-4f78-9093-5d155176bffd",
   "metadata": {},
   "source": [
    "- First we need to set the range of runs that we want to enclose in the final dataframe, it can be also a single run, but it needs to be iterable in order for the following code to properly work.\n",
    "- Moreover, we need to specify the feature we need as columns in the dataframe, this is done by modifying the `keys_to_save` list.\n",
    "- Lastly, we specify the path where the reco files are stored, together with the path where we want to save the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da23f8e8-c0d0-4a6f-a016-ff871260d0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepairing to load 3 runs...\n"
     ]
    }
   ],
   "source": [
    "##TEST\n",
    "runs = np.arange(34877, 34880, 1)\n",
    "\n",
    "print(f\"Prepairing to load {len(runs)} runs...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06bf3a83-0fc1-4d9d-b3ad-340cf0214a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_save = ['run', 'event', 'pedestal_run', 'cmos_integral', 'cmos_mean',\n",
    "                'cmos_rms', 't_DBSCAN', 't_variables', 'lp_len', 't_pedsub',\n",
    "                't_saturation', 't_zerosup', 't_xycut', 't_rebin', 't_medianfilter',\n",
    "                't_noisered', 'nSc', 'sc_size', 'sc_nhits', 'sc_integral',\n",
    "                'sc_corrintegral', 'sc_rms', 'sc_energy', 'sc_pathlength',\n",
    "                'sc_redpixIdx', 'nRedpix', 'sc_theta', 'sc_length', 'sc_width',\n",
    "                'sc_longrms', 'sc_latrms', 'sc_lfullrms', 'sc_tfullrms',\n",
    "                'sc_lp0amplitude', 'sc_lp0prominence', 'sc_lp0fwhm', 'sc_lp0mean',\n",
    "                'sc_tp0fwhm', 'sc_xmean', 'sc_ymean', 'sc_xmax', 'sc_xmin', 'sc_ymax',\n",
    "                'sc_ymin', 'sc_pearson', 'sc_tgaussamp', 'sc_tgaussmean',\n",
    "                'sc_tgausssigma', 'sc_tchi2', 'sc_tstatus', 'sc_lgaussamp',\n",
    "                'sc_lgaussmean', 'sc_lgausssigma', 'sc_lchi2', 'sc_lstatus'#,\n",
    "                #'Lime_pressure', 'Atm_pressure', 'Lime_temperature', 'Atm_temperature',\n",
    "                #'Humidity'\n",
    "               ]\n",
    "\n",
    "# keys_to_save = ['run', 'event', 'nSc', 'sc_nhits', 'sc_integral', 'sc_energy', 'sc_length', \n",
    "#                 'sc_xmean', 'sc_ymean', 'sc_xmax', 'sc_xmin', 'sc_ymax', 'sc_ymin',\n",
    "#                ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d869b287-1311-4182-8344-0d6a14e5aca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "recopath = '/jupyter-workspace/cloud-storage/cygno-analysis/RECO/Winter23/'\n",
    "savepath = '/jupyter-workspace/cloud-storage/zappater/CMOSEvents/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a3a978-6f3e-4f0a-81df-197af95449f5",
   "metadata": {},
   "source": [
    "## Create Dataframe\n",
    "<a id = 'df'></a>\n",
    "\n",
    "Go to [TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28089f14-102b-474e-95ab-d3fb74d774fe",
   "metadata": {},
   "source": [
    "First of all we need to download the most recent logbook (e.g. from Grafana) and put it in the same folder of the code, such that we can retrieve important information about the considered run, useful for example to esclude pedestal runs from the dataframe (if we want to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0bc4bc6-bd3d-40bf-9edc-936025173ccd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'logbook.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9ef76fccb7d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## load logbook, 'low_memory=False' option is to avoid an annoying warning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdfinfo_tot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'logbook.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'logbook.csv'"
     ]
    }
   ],
   "source": [
    "## load logbook, 'low_memory=False' option is to avoid an annoying warning\n",
    "## the one in this folder is update up to 2024-01-05 14:27:18\n",
    "dfinfo_tot = pd.read_csv('logbook.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be51cbb0-f8c8-4b5c-9c99-626d4a62c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data in the following range was taken with HV off but not marked as pedestal and turning HV on from time to time as explained later, in the code I escluded data with HV on in this\n",
    "# range of runs, since it only creates noise due to the Iron source spots.\n",
    "\n",
    "runs_ped = np.arange(20426, 20816, 1) # line 22 of the RUN3-summary in the datalogbook sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfbb581e-b03c-43bf-9c46-517204bb96dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91cba685d19e446fb263a8304c8982c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'dfinfo_tot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d04a9753559d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# extract the part of the logbook relative to the current run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdfinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfinfo_tot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdfinfo_tot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'run_number'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m## there is a particular set of pedestal runs where the HV was turned on from time to time to check the GEM response (line 22 of the RUN3-summary in the datalogbook sheet),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfinfo_tot' is not defined"
     ]
    }
   ],
   "source": [
    "## OPEN RECO FILES AND SAVE TO CSV\n",
    "\n",
    "## the code is written with exceptions required to prevent crashing when encountering zombies/corrupted or missing files,\n",
    "## since it is interesting to know which files are like that, we store them in two lists.\n",
    "not_found = []\n",
    "corrupted = []\n",
    "\n",
    "df_RUN3 = pd.DataFrame()\n",
    "\n",
    "## loop over the run range\n",
    "for r in tqdm(runs):\n",
    "    # create the filename to be read\n",
    "    filename = f\"{recopath}reco_run{int(r)}_3D.root\"\n",
    "    #dfinfo = cy.run_info_logbook(r, sql=True, verbose=False) # this was here when Stefano sent me this code, but I never tried to implement this line of code\n",
    "    \n",
    "    # extract the part of the logbook relative to the current run\n",
    "    dfinfo = dfinfo_tot[dfinfo_tot['run_number']==r].copy()\n",
    "    \n",
    "    ## there is a particular set of pedestal runs where the HV was turned on from time to time to check the GEM response (line 22 of the RUN3-summary in the datalogbook sheet), \n",
    "    ## in that case the interesting files are the pedestals with HV-off, and the HV-on runs should be excluded using the two following lines:\n",
    "    if (r==runs_ped).any()&(dfinfo['HV_STATE'].values[0]==1):\n",
    "        continue\n",
    "    \n",
    "    # only when we encounter actual data runs (pedestal_run=0) we proceed in opening and saving the files.\n",
    "    if dfinfo['pedestal_run'].values[0]==0:#and os.path.exists(filename) and# and dfinfo['source_type'].values[0]==0: \n",
    "        print('Opening reco file of run {:05d}...'.format(r), end = '\\r')\n",
    "        try:\n",
    "            file = uproot.open(filename+\":Events\")\n",
    "        except:\n",
    "            print(\"\\nAAAAA\", r)\n",
    "            not_found.append(r)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            datadf = file.arrays(keys_to_save, library=\"pd\")\n",
    "        except:\n",
    "            print(\"\\nBBBBB\", r)\n",
    "            corrupted.append(r)\n",
    "            continue\n",
    "        \n",
    "        # at the end of each iteration we concatenate the current dataframe to the big one and we delete the first from the memory.\n",
    "        df_RUN3 = pd.concat([df_RUN3, datadf], ignore_index=True)\n",
    "        del datadf\n",
    "    #print(len(df_RUN3))\n",
    "        \n",
    "\n",
    "# at this point we compress the file and then we save it to a zip in the selected location.\n",
    "compression_opts = dict(method='zip', archive_name='./dfRUN3.csv')  \n",
    "df_RUN3.to_csv(f'{savepath}dfRUN3_{runs[0]}_{runs[-1]}_test.zip', index=False, compression=compression_opts)\n",
    "print(\"\\nEverything was saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e4ef98-1841-4148-94ed-2264aa63ed77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729ccc2-8290-49e3-8c6d-9716192f9427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
